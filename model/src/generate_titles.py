#!/usr/bin/env python3

import json
import openai
from retry import retry
import time

model="ada:ft-TODO"

@retry(Exception, tries=3, delay=10)
def abstract_to_title(prompt, model="ada:ft-eth-z-rich-2022-09-28-20-17-30", max_tokens=25, n=5):
    prediction = openai.Completion.create(
        model=model,     
        prompt=prompt + " \n\n###\n\n",
        max_tokens=max_tokens,
        stop=[" ###"],
        logprobs=0,
        top_p=1,
        n=n,
    )
  
    titles = [choice["text"].strip() for choice in prediction["choices"]]
#   titles_logprobs = [np.exp(np.sum(choice["logprobs"]["token_logprobs"])) for choice in prediction["choices"]]
#   predicted_reason = prediction["choices"][0]["text"].strip()
  
    return titles, prediction

ABSTRACTS_TITLES = [
    ("Leveraging machine translation for cross-lingual fine-grained cyberbullying classification amongst pre-adolescents", "Cyberbullying is the wilful and repeated infliction of harm on an individual using the Internet and digital technologies. Similar to face-to-face bullying, cyberbullying can be captured formally using the Routine Activities Model (RAM) whereby the potential victim and bully are brought into proximity of one another via the interaction on online social networking (OSN) platforms. Although the impact of the COVID-19 (SARS-CoV-2) restrictions on the online presence of minors has yet to be fully grasped, studies have reported that 44% of pre-adolescents have encountered more cyberbullying incidents during the COVID-19 lockdown. Transparency reports shared by OSN companies indicate an increased take-downs of cyberbullying-related comments, posts or content by artificially intelligen moderation tools. However, in order to efficiently and effectively detect or identify whether a social media post or comment qualifies as cyberbullying, there are a number factors based on the RAM, which must be taken into account, which includes the identification of cyberbullying roles and forms. This demands the acquisition of large amounts of fine-grained annotated data which is costly and ethically challenging to produce. In addition where fine-grained datasets do exist they may be unavailable in the target language. Manual translation is costly and expensive, however, state-of-the-art neural machine translation offers a workaround. This study presents a first of its kind experiment in leveraging machine translation to automatically translate a unique pre-adolescent cyberbullying gold standard dataset in Italian with fine-grained annotations into English for training and testing a native binary classifier for pre-adolescent cyberbullying. In addition to contributing high-quality English reference translation of the source gold standard, our experiments indicate that the performance of our target binary classifier when trained on machine-translated English output is on par with the source (Italian) classifier.")
    ("Sequential or jumping: context-adaptive response generation for open-domain dialogue systems", "Neural response generation can automatically produce replies for open-domain dialogue systems without hand-crafted rules or templates. Current studies follow a non-context-adaptive paradigm that employs a single response generator to deal with all dialogues. However, as a dialogue progresses, its textual characteristics (e.g., context length, information volume, involving topics) are changing, so are the issues challenging its response generation. Non-context-adaptive response generators are inflexible and may fail to achieve globally good performance without considering the differences existing among dialogues. In this paper, we propose a novel framework named as C ontext-A daptive R esponse G eneration (CARG), which assembles two different response generators to respectively handle long and short dialogues. Specifically, given a dialogue, CARG first classifies it into short or long types according to the number of its containing utterances. For a short dialogue, CARG employs a sequential reader (SR) to concatenates all utterances into a sequence aiming to construct the dialogue context by limited semantics. For a long dialogue where irrelevant noises and relevant contexts both exist, CARG uses a jumping reader (JR) to generate the response, which treats the latest utterance as the anchor and further performs selective context utilization under its guidance. We introduce ensemble learning strategy to conduct the training and testing of CARG. Extensive experimental results on two benchmark chat corpora show that the proposed CARG framework can outperform various competitive baselines, validating its effectiveness on response generation."),
    ("Multilingual Transformer Language Model for Speech Recognition in Low-resource Languages", "It is challenging to train and deploy Transformer LMs for hybrid speech recognition 2nd pass re-ranking in low-resource languages due to (1) data scarcity in low-resource languages, (2) expensive computing costs for training and refreshing 100+ monolingual models, and (3) hosting inefficiency considering sparse traffic. In this study, we present a new way to group multiple low-resource locales together and optimize the performance of Multilingual Transformer LMs in ASR. Our Locale-group Multilingual Transformer LMs outperform traditional multilingual LMs along with reducing maintenance costs and operating expenses. Further, for low-resource but high-traffic locales where deploying monolingual models is feasible, we show that fine-tuning our locale-group multilingual LMs produces better monolingual LM candidates than baseline monolingual LMs."),
    ("Adapting to Non-Centered Languages for Zero-shot Multilingual Translation", "Multilingual neural machine translation can translate unseen language pairs during training, i.e. zero-shot translation. However, the zero-shot translation is always unstable. Although prior works attributed the instability to the domination of central language, e.g. English, we supplement this viewpoint with the strict dependence of non-centered languages. In this work, we propose a simple, lightweight yet effective language-specific modeling method by adapting to non-centered languages and combining the shared information and the language-specific information to counteract the instability of zero-shot translation. Experiments with Transformer on IWSLT17, Europarl, TED talks, and OPUS-100 datasets show that our method not only performs better than strong baselines in centered data conditions but also can easily fit non-centered data conditions. By further investigating the layer attribution, we show that our proposed method can disentangle the coupled representation in the correct direction."),
    ("Asynchronous haltere input impairs wing and gaze control in Drosophila", "Halteres are multifunctional mechanosensory organs unique to the true flies (Diptera). A set of reduced hindwings, the halteres beat at the same frequency as the lift-generating forewings and sense inertial forces via mechanosensory campaniform sensilla. Though it is well-established that haltere ablation makes stable flight impossible, the specific role of wing-synchronous input has not been established. Using small iron filings attached to the halteres of tethered flies and an alternating electromagnetic field, we experimentally decoupled the wings and halteres of flying Drosophila and observed the resulting changes in wingbeat amplitude and head orientation. We find that providing asynchronous haltere input drives fast saccades in the wing, and that wing and gaze optomotor responses are disrupted differently by asynchronous input. Our findings help to synthesize our understanding of the behavioral deficits that accompany haltere ablation with the known physiology of both haltere neurons and wing and neck muscles."),
]

for orig_title, abstract in ABSTRACTS_TITLES:
    titles, results = abstract_to_title(abstract, n=10)
    print(json.dumps({"abstract": abstract, "titles": [orig_title] + titles}))
    input("Press ENTER to continue")